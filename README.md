# üõ†Ô∏è Web Scraping + Hadoop MapReduce

Projeto que realiza scraping de dados da web usando Python (Selenium) e os processa com Hadoop MapReduce. Tudo automatizado com Docker e Docker Compose.

---

## üéì Informa√ß√µes Acad√™micas

Este projeto faz parte da disciplina de **Big Data** do curso de **Especializa√ß√£o em Engenharia de Software com DevOps** da **Universidade Estadual do Cear√° - UECE**.

- **Alunos:** Ezemir Sabino e Marcos Eduardo  
- **Professor:** Denis Sousa

---

## üöÄ Tecnologias Utilizadas

- **Python 3 + Selenium**: para web scraping  
- **Apache Hadoop (HDFS + YARN + MapReduce)**: para processamento distribu√≠do  
- **Java**: job MapReduce compilado em um `.jar`  
- **Docker + Docker Compose**: para orquestra√ß√£o dos servi√ßos  

---

## üì¶ Etapas do Processo

### 1. Web Scraping

Antes de subir os containers, execute o script de scraping:

```bash
python webscraping.py
```

Esse script salva arquivos `.csv` na pasta `/data/csv`.

---

### 2. Subir o ambiente completo

Tudo √© automatizado a partir daqui. Basta rodar:

```bash
docker-compose up --build
```

Isso ir√°:

- Subir os servi√ßos Hadoop (NameNode, DataNode, ResourceManager etc)
- Criar automaticamente diret√≥rios no HDFS
- Carregar os arquivos CSV gerados pelo scraper
- Executar o Job Hadoop MapReduce (`job.jar`)
- Salvar os resultados em `/data/output` no HDFS

---

## üìÅ Estrutura Esperada no HDFS

```
/data/input     ‚Üê arquivos CSV carregados  
/data/output    ‚Üê resultado do Job MapReduce
```

---

## üìå Observa√ß√µes

- O `geckodriver.exe` j√° est√° inclu√≠do no projeto. Apenas certifique-se de ter o Firefox instalado.  
- O Job Java j√° est√° compilado como `job.jar` e √© executado automaticamente pelo servi√ßo `init-hdfs`.  
- As configura√ß√µes do Hadoop est√£o na pasta `config/`.

---

## ‚úÖ Verificando os Resultados

Voc√™ pode acessar a UI do Hadoop para acompanhar o processamento:

- **NameNode UI:** http://localhost:9870  
- **ResourceManager UI:** http://localhost:8088  
- **HistoryServer:** http://localhost:8188

---

## üîÅ Reexecutar o Job

Para rodar o job novamente (por exemplo, ap√≥s novo scraping), basta:

```bash
docker-compose restart init-hdfs
```

---

## üìÑ Licen√ßa

Distribu√≠do sob a licen√ßa MIT. Livre para usar, estudar e modificar.

---

> Projeto criado para fins did√°ticos envolvendo Big Data, automa√ß√£o e processamento distribu√≠do com MapReduce.